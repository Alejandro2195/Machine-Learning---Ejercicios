---
title: "Machine Learning en R - Parte 5"
author: "Alejandro José Gómez García"
date: "15/5/2022"
output: html_document
---
Temas tratados en este documento:
 - Modelo de máquina de vectores de soporte (SVM Model)
 - 
 
```{r}
#Paquetes necesarios:
library(caTools)
library(dummies)
library(leaps)
library(glmnet)
library(foreach)
library(MASS)
library(class)
library(rpart)
library(rpart.plot)
library(e1071)
library(gbm)
library(adabag)
library(xgboost)
library(randomForest)
library(caret)
```

Creación del modelo de máquina de vectores de soporte en R
```{r}
#Importamos los datos y utilizando la función summary vemos que hay NA en la variable Tm_taken
movie <- read.csv("C:/Users/Shiro/Desktop/Datos/Movie_classification.csv", header = TRUE)
summary(movie)

#Porcedemos a eliminar los NA de Time_taken y sustituirlos por la media de dicha variable
movie$Time_taken[is.na(movie$Time_taken)]<-mean(movie$Time_taken, na.rm=TRUE)

#Ahora dividimos nuestros datos en dos: una parte para entrenamiento y la otra para prueba (train y test), para ello necesitamos el paquete caTools

set.seed(0)
split=sample.split(movie,SplitRatio = 0.8)  #dividimos los datos en dos partes, en una proporción de 8 a 2, en realidad split es una varibale que almacena solamente el 80% de los datos
train=subset(movie,split==TRUE)     #creamos el subset para el entrenamiento, split==TRUE significa que nos quedamos con lo que almacenó la variable split (80% de los datos)
test=subset(movie,split==FALSE)

```

MÁQUINAS DE VECTORES DE SOPORTE (SVM Model) - en este caso utilizando una función kernel

Dado un conjunto de puntos, subconjunto de un conjunto mayor (espacio), en el que cada uno de ellos pertenece a una de dos posibles categorías, un algoritmo basado en SVM construye un modelo capaz de predecir si un punto nuevo (cuya categoría desconocemos) pertenece a una categoría o a la otra. 

La SVM busca un hiperplano que separe de forma óptima a los puntos de una clase de la de otra, que eventualmente han podido ser previamente proyectados a un espacio de dimensionalidad superior. 

 - El valor mínimo de DISTANCIA PERPENDICULAR entre las observaciones y el hiperplano se llama MARGEN
 - El hiperplano que mejor divide a los grupos de datos será el que tenga un mayor margen
 - Al vector formado por los puntos más cercanos al hiperplano (cuya distancia perpendicular=margen) se le llama vector de soporte.

Los modelos basados en SVM están estrechamente relacionados con las redes neuronales. Usando una función kernel, resultan un método de entrenamiento alternativo para clasificadores polinomiales, funciones de base radial y perceptrón multicapa. 

```{r}
#Para la clasificación
train$Start_Tech_Oscar<-as.factor(train$Start_Tech_Oscar)
test$Start_Tech_Oscar<-as.factor(test$Start_Tech_Oscar)

#Entrenamos las SVM con el set de entrenamiento
svmfit=svm(Start_Tech_Oscar~., data = train, kernel="linear", cost=1, scale=TRUE)
summary(svmfit)
```

El parámetro de costo (cost) penaliza los grandes residuos. Por lo tanto, un costo mayor resultará en un modelo más flexible con menos clasificaciones erróneas. En efecto, el parámetro de costo le permite ajustar la compensación de sesgo / varianza. Cuanto mayor sea el parámetro de costo, habrá más varianza en el modelo y menos sesgo y por tanto menos clasificaciones erróneas. El parámetro de costo básicamente modifica la amplitud de los márgenes, a menor costo márgenes más amplios y viceversa

```{r}
#Predecimos utilizando el set de prueba
ypred=predict(svmfit, test)
table(predict=ypred, original=test$Start_Tech_Oscar)

#Chequeamos los vectores de soporte
svmfit$index
```

Cómo encontrar el mejor valor para Cost:
utilizamos una función que pruebe los modelos con diferentes valores de cost
```{r}
set.seed(0)
tune.out=tune(svm, Start_Tech_Oscar~., data=train, kernel="linear", ranges = list(cost=c(0.001,0.01,0.1,1,10,100)))

#buscamos el mejor modelo, podemos buscar el rendimiento de cada modelo o los mejores parámetros
bestmod=tune.out$best.model
summary(bestmod)

predfinal=predict(bestmod, test)
table(predict=predfinal, original=test$Start_Tech_Oscar)

```

Kernel Polinomial

```{r}
svmfitP=svm(Start_Tech_Oscar~., data = train, kernel="polynomial", cost=10, degree=1)
summary(svmfitP)
predP =predict(svmfitP, test)
table(predict=predP, original=test$Start_Tech_Oscar)

#utilizamos una función que pruebe los modelos con diferentes valores de cost
tune.outP = tune(svm, Start_Tech_Oscar~., data = train, cross=4, kernel="polynomial", ranges = list(cost=c(0.001,0.1,1,5,10,15,20), degree=c(0.5,1,2,3,5)))
bestmodP= tune.outP$best.model
summary(bestmodP)

predP =predict(bestmodP, test)
table(predict=predP, original=test$Start_Tech_Oscar)
```

Kernel Radial

```{r}
svmfitR=svm(Start_Tech_Oscar~., data = train, kernel="radial", cost=10, gamma=1)
summary(svmfitR)
predR =predict(svmfitR, test)
table(predict=predR, original=test$Start_Tech_Oscar)

#utilizamos una función que pruebe los modelos con diferentes valores de cost
tune.outR = tune(svm, Start_Tech_Oscar~., data = train, cross=4, kernel="radial", ranges = list(cost=c(0.001,0.01,0.1,1,10,100,1000), gamma=c(0.01,0.1,0.5,1,3,10,50)))
bestmodR= tune.outR$best.model
summary(bestmodR)

predR =predict(bestmodR, test)
table(predict=predR, original=test$Start_Tech_Oscar)
 
```
